## Social Network Analytics Pipeline using Spark and Airflow

### Task

Launch a daily pipeline for a fictional social network in a Hadoop HDFS-based data lake, performing the following tasks as per requirements. Document the schema of the data marts and the pipeline files in the data catalog.

1. Incremental loading of event logs (messages, reactions, subscriptions) from the raw data layer to ODS.
2. Building a data mart with user activity statistics by city.
3. Building a data mart with event statistics by city.
4. Building a data mart for friend recommendations (common subscriptions to 1+ channel & distance less than 1 km).

### Skills  
ETL pipeline development using Airflow and Spark, building a Data Lake on Hadoop HDFS, incremental loading, data mart modeling, big data processing with Python, automation of calculations, testing DAGs in Airflow, documenting solutions.
 

## Пайплайн для аналитики соцсети с использованием Spark и Airflow

### Задача

Запустить ежедневный пайплайн для условной социальной сети в озере данных на базе Hadoop HDFS, выполняющий задачи согласно требованиям. Задокументировать схемы витрин и файлы пайплайна в дата-каталоге.

1. Инкрементальная загрузка логов событий (сообщения, реакции, подписки) из слоя сырых данных в ODS.
2. Сборка витрины с активностью пользователей по городам.
3. Сборка витрины со статистикой событий по городам.
4. Сборка витрины для рекомендации друзей (общая подписка на 1+ канал и расстояние менее 1 км).

### Навыки  
Разработка ETL-пайплайна на Airflow и Spark, построение Data Lake на Hadoop HDFS, инкрементальная загрузка, моделирование витрин, обработка больших данных на Python, автоматизация расчётов, тестирование DAG в Airflow, документирование решений.
